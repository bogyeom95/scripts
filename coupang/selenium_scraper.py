import os
import csv
import time
import random
import re
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
import undetected_chromedriver as uc
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from .enums import FilterType
import logging



logging.basicConfig(level=logging.INFO, format='[%(levelname)s][%(asctime)s][%(name)s]: %(message)s')
log = logging.getLogger(__name__)




class CoupangSeleniumScraper:
    def __init__(self, keyword, filter_type=FilterType.ROCKET, max_page=1, headless=True):
        self.keyword = keyword
        self.filter_type = filter_type
        self.max_page = max_page
        self.results = []
        self.base_url = "https://www.coupang.com/np/search?q="
        self.headless = headless    
        

    def _init_driver(self):
        options = uc.ChromeOptions()
        options.add_argument("--window-size=1920,1080")

        
        driver = uc.Chrome(options=options, use_subprocess=True, headless=self.headless)
        return driver

    def scrape(self):
        driver = self._init_driver()
        log.info(f"[{self.keyword}] í¬ë¡¤ë§ ì‹œì‘... (ì´ {self.max_page}í˜ì´ì§€)")
        
        try:
            for page in range(1, self.max_page + 1):
                url = f"{self.base_url}{self.keyword}&filterType={self.filter_type}&page={page}"
                driver.get(url)
                
            
                try:
                    # ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìƒí’ˆ ë‹¨ìœ„ì˜ li íƒœê·¸ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€)
                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'li[class*="ProductUnit"]'))
                    ) # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
                    # ë´‡ íƒì§€ ìš°íšŒë¥¼ ìœ„í•œ ëœë¤ ë”œë ˆì´ (ìƒí’ˆì´ ë¡œë“œëœ í›„ ì¶”ê°€ë¡œ ëŒ€ê¸°)
                    time.sleep(random.uniform(0.5, 1.5))
                except TimeoutException:
                    log.warning(f"[{page}í˜ì´ì§€] ìƒí’ˆ ë¡œë”© ì‹œê°„ ì´ˆê³¼. í˜ì´ì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                
                self._parse_html(driver.page_source)
                log.info(f"[{self.keyword}] {page}í˜ì´ì§€ ì™„ë£Œ")
                
                
                
                # í˜ì´ì§€ë¥¼ ë„˜ê¸°ê¸° ì „ ë”œë ˆì´ëŠ” ë´‡ íƒì§€ íšŒí”¼ë¥¼ ëª©ì 
                time.sleep(random.uniform(2.0, 4.0))
                
        except Exception as e:
            
            log.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            today = time.strftime("%Y%m%d_%H%M%S")
           
        
            if not os.path.exists(f"error_screenshots/{self.keyword}"):
                os.makedirs(f"error_screenshots/{self.keyword}")
            driver.save_screenshot(f"error_screenshots/{self.keyword}/error_{today}.png")
        finally:
            driver.quit()
            
        return self.results
  
    def _parse_html(self, html_source):
        soup = BeautifulSoup(html_source, 'html.parser')
        items = soup.select('li[class*="ProductUnit"]')
        
        for item in items:
            # 1. ìƒí’ˆëª… ì¶”ì¶œ
            name_tag = item.select_one('div[class*="productName"]')
            name = name_tag.text.strip() if name_tag else "ìƒí’ˆëª… ì—†ìŒ"
            
            if name == "ìƒí’ˆëª… ì—†ìŒ":
                continue 
                
            # 2. ê°€ê²© ì¶”ì¶œ
            price_area = item.select_one('div[class*="PriceArea"]')
            price = "0"
            unit_price_text = "ì •ë³´ ì—†ìŒ"
            
            if price_area:
                # ê°€ê²© ì¶”ì¶œ (ì˜ˆ: 70,000ì› -> 70000)
                match = re.search(r'([0-9,]+)ì›', price_area.text)
                if match:
                    price = match.group(1).replace(',', '')
                
                # ì¶”ê°€1: ë‹¨ìœ„ë‹¹ ê°€ê²© ì¶”ì¶œ (ì˜ˆ: (10gë‹¹ 309ì›))
                unit_match = re.search(r'\((.*?ë‹¹\s*[0-9,]+ì›)\)', price_area.text)
                if unit_match:
                    unit_price_text = unit_match.group(1)

            # ì¶”ê°€2: í’ˆì ˆ ì—¬ë¶€ í™•ì¸ ("í’ˆì ˆ"ì´ë¼ëŠ” ê¸€ìê°€ í…ìŠ¤íŠ¸ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸)
            is_sold_out = "O" if "í’ˆì ˆ" in item.text else "X"

            # ì¶”ê°€3: ìƒí’ˆ ì¤‘ëŸ‰ ì¶”ì¶œ (ìƒí’ˆëª…ì—ì„œ kg, g, ml, l ë“±ì˜ íŒ¨í„´ì„ ì°¾ìŒ)
            weight = "ì •ë³´ ì—†ìŒ"
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ ê²€ìƒ‰
            weight_matches = re.findall(r'([0-9.]+\s?(?:kg|g|ml|l|oz|lbs))', name, re.IGNORECASE)
            if weight_matches:
                weight = weight_matches[-1] # ë³´í†µ ìƒí’ˆëª… ë§¨ ë’¤ì— ìˆëŠ” ë‹¨ìœ„ê°€ ì´ ì¤‘ëŸ‰ì…ë‹ˆë‹¤.

            # 4. í‰ì  ì¶”ì¶œ
            rating_tag = item.select_one('div[aria-label]')
            rating = rating_tag['aria-label'] if rating_tag else "0.0"

            # 5. ë¦¬ë·° ìˆ˜ ì¶”ì¶œ
            review_count = "0"
            rating_area = item.select_one('div[class*="ProductRating"]')
            if rating_area:
                match = re.search(r'\(([0-9,]+)\)', rating_area.text)
                if match:
                    review_count = match.group(1).replace(',', '')
            
            # 6. ë§í¬ ì¶”ì¶œ
            link_tag = item.select_one('a[href*="/vp/products"]')
            link = f"https://www.coupang.com{link_tag['href']}" if link_tag else ""

            # ì¶”ì¶œí•œ ëª¨ë“  ë°ì´í„°ë¥¼ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ìŠµë‹ˆë‹¤.
            self.results.append({
                'ìƒí’ˆëª…': name,
                'ê°€ê²©(ì›)': int(price) if price.isdigit() else 0,
                'ë‹¨ìœ„ë‹¹ê°€ê²©': unit_price_text,
                'ì¤‘ëŸ‰': weight,
                'í’ˆì ˆì—¬ë¶€': is_sold_out,
                'í‰ì ': float(rating) if rating.replace('.', '', 1).isdigit() else 0.0,
                'ë¦¬ë·°ìˆ˜': int(review_count) if review_count.isdigit() else 0,
                'ìƒí’ˆë§í¬': link
            })

    def save_to_csv(self, folder_path, filename):
        if not self.results: 
            log.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
            return
            
        keys = self.results[0].keys()
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        with open(os.path.join(folder_path, filename), 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(self.results)

        log.info(f"ğŸ‰ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {os.path.join(folder_path, filename)} (ì´ {len(self.results)}ê°œ ìƒí’ˆ)")
        
        
